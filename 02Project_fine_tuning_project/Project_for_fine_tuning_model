{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZerocoderUniversity/AI-course-prompt-engineering/blob/main/Project_for_fine_tuning_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"align-center\"> <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a> <a href=\"https://ollama.com/\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/ollama.png\" height=\"44\"></a>\n"
      ],
      "metadata": {
        "id": "zv8tk44A1pUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"zerocodemethodist@gmail.com\"\n",
        "!git config --global user.name \"ZerocoderForstudents\"\n",
        "!git add .\n",
        "!git commit -m \"Сообщение коммита\"\n",
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6NvkUGn0Kqi",
        "outputId": "7bfbbbd5-c53f-4c66-b012-f51fc71fae14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Последняя сборка\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Поддержка Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes и другие модели.\n",
        "* Поддержка 16-битный LoRA и 4-битный QLoRA. Оба варианта в 2 раза быстрее.\n",
        "* Параметр max_seq_length можно установить любой, поскольку используется автоматическое масштабирование RoPE с помощью метода [kaiokendev's](https://kaiokendev.github.io/til)\n",
        "\n",
        "* [НОВОЕ] Ускорена работа Phi-3 Medium / Mini в 2 раза!"
      ],
      "metadata": {
        "id": "030OYCyH4fCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048  # Выберите любое значение! поддержка масштабирования RoPE.\n",
        "dtype = None  # None для автоопределения. Float16 для Tesla T4, V100, Bfloat16 для Ampere+.\n",
        "load_in_4bit = True  # Использовать 4-битное квантование для уменьшения использования памяти. Может быть False.\n",
        "\n",
        "# список доступных моделей\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # Новая Mistral v3, в 2 раза быстрее!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3, в 2 раза быстрее!\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3, в 2 раза быстрее!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma, в 2.2 раза быстрее!\n",
        "]  # Больше моделей можно найти на https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # Используйте токен, если работаете с моделями, которые требуют gated access (например, meta-llama/Llama-2-7b-hf)\n",
        ")\n"
      ],
      "metadata": {
        "id": "4KuenCh45Vpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавляем LoRA, чтобы обновлять часть весов"
      ],
      "metadata": {
        "id": "xxwNBjBnBNur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,  # Выберите любое число > 0! Рекомендуемые значения: 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,  # Поддерживает любые значения, но 0 оптимально\n",
        "    bias = \"none\",     # Поддерживает любые значения, но \"none\" оптимально\n",
        "    # [НОВОЕ] \"unsloth\" использует на 30% меньше VRAM, позволяет использовать в 2 раза большие размеры батчей!\n",
        "    use_gradient_checkpointing = \"unsloth\",  # True или \"unsloth\" для очень длинного контекста\n",
        "    random_state = 3407,\n",
        "    # use_rslora = False,  # Поддержка rank stabilized LoRA\n",
        "    # loftq_config = None,  # И LoftQ\n",
        ")\n"
      ],
      "metadata": {
        "id": "5My9pIiS68Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "### Подготовка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используем набор данных с HF, он уже представлен в необходимом виде и явлется объектом класса datasets\n",
        "\n",
        "[Набор данных](https://huggingface.co/datasets/Amod/mental_health_counseling_conversations?row=1) представляет собой коллекцию вопросов и ответов, взятых с двух платформ для онлайн-консультирования и терапии. Вопросы охватывают широкий спектр тем, связанных с психическим здоровьем, а ответы даны квалифицированными психологами.\n",
        "\n",
        "Набор данных предназначен для тонкой настройки языковых моделей с целью улучшения их способности предоставлять консультации по вопросам психического здоровья.\n",
        "\n",
        "Это может пригодиться при создании чат-ботов, оказывающих консультации людям\n"
      ],
      "metadata": {
        "id": "9FUpuw0PBa1H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvOPfPnet76H"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"Amod/mental_health_counseling_conversations\", split = \"train\")\n",
        "print(dataset.column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Также разделим датасет на две части - обучающую и тестовую выборки"
      ],
      "metadata": {
        "id": "mZ47M2kZCdsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделяем датасет на train и test\n",
        "train_test_split = dataset.train_test_split(test_size=0.2)  # 20% данных на тестовый набор\n",
        "\n",
        "train_dataset = train_test_split['train']\n",
        "test_dataset = train_test_split['test']\n",
        "\n",
        "print(\"Train dataset size:\", len(train_dataset))\n",
        "print(\"Test dataset size:\", len(test_dataset))"
      ],
      "metadata": {
        "id": "Xt_ClEqp1abZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg4_dG-m0Cz4"
      },
      "source": [
        "Может возникунть проблема, которая заключается в том, что некоторые наборы данных содержит несколько столбцов. Для того чтобы Ollama и llama.cpp работали как чат-бот ChatGPT, необходимо, чтобы в наборе данных было только два столбца — instruction (инструкция) и output (выход)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для решения этой задачи мы сделаем следующее:\n",
        "\n",
        "* Объединим все столбцы в один prompt.\n",
        "* Используем функцию *to_sharegpt*, чтобы выполнить процесс слияния столбцов!"
      ],
      "metadata": {
        "id": "RH3uLBks8K71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы объединить несколько столбцов в один, используйте **merged_prompt**.\n",
        "\n",
        "* Заключите все столбцы в фигурные скобки {}.\n",
        "* Необязательный текст должен быть заключен в [[]]. Например, если столбец \"Pclass\" пуст, функция слияния не покажет этот текст и пропустит его. Это полезно для наборов данных с отсутствующими значениями.\n",
        "* Вы можете выбрать все столбцы или несколько!\n",
        "* Выберите столбец с предсказаниями в output_column_name. Для нашего набора данных это - Response\n",
        "* *Дополнительно*: Чтобы дообучение поддерживало несколько обменов сообщениями (как в ChatGPT), нужно создать \"фейковый\" набор данных с несколькими сообщениями в диалоге — для этого мы используем conversation_extension, чтобы случайным образом выбрать несколько разговоров из набора данных и объединить их в один разговор"
      ],
      "metadata": {
        "id": "217KJxDA8aJ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZxeGSeX0CR8"
      },
      "outputs": [],
      "source": [
        "from unsloth import to_sharegpt\n",
        "dataset = to_sharegpt(\n",
        "    dataset,\n",
        "    merged_prompt = \"{Context}\",\n",
        "    output_column_name = \"Response\",\n",
        "    # conversation_extension = 1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kh90vpD1jYJ"
      },
      "source": [
        "Используйте **standardize_sharegpt**, чтобы привести набор данных к нужному виду!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPwDXBvP1g8S"
      },
      "outputs": [],
      "source": [
        "from unsloth import standardize_sharegpt\n",
        "dataset = standardize_sharegpt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GThrcKACxTe2"
      },
      "source": [
        "### Настройка шаблона (промпта) чата"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOGaZf1sdLlr"
      },
      "outputs": [],
      "source": [
        "chat_template = \"\"\"Below are some conversations. Write responses that appropriately complete each request.\n",
        "\n",
        "### Question:\n",
        "{INPUT}\n",
        "\n",
        "### Corresponding answer:\n",
        "{OUTPUT}\"\"\"\n",
        "\n",
        "from unsloth import apply_chat_template\n",
        "dataset = apply_chat_template(\n",
        "    dataset,\n",
        "    tokenizer = tokenizer,\n",
        "    chat_template = chat_template,\n",
        "    # default_system_message = \"You are a helpful assistant\", << [OPTIONAL]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "R6kMC_Iy4L0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим, как модель отвечает на вопросы до дообучения"
      ],
      "metadata": {
        "id": "uwhpA2mjDP25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Ускорение модели\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here. I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it. How can I change my feeling of being worthless to everyone?\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Время начала генерации\n",
        "start_time = time.time()\n",
        "# Генерируем текст\n",
        "output_ids = model.generate(input_ids, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# Время окончания генерации\n",
        "end_time = time.time()\n",
        "\n",
        "# Время, затраченное на генерацию\n",
        "generation_time = end_time - start_time\n",
        "print(f\"Время генерации: {generation_time:.2f} секунд\")"
      ],
      "metadata": {
        "id": "N1tQyH0W8ct6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Декодируем токены в строку\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "rAQVweYKhv7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Декодируем токены в строку\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "cbQEaG0t9Rpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_ids)"
      ],
      "metadata": {
        "id": "UgM0yisA80a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['text'][1])  # Покажет первые 10 элементов из столбца 'text'"
      ],
      "metadata": {
        "id": "Hb-iwup_e2I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Дообучение модели\n",
        "\n"
      ],
      "metadata": {
        "id": "pbiPqHe6D7n6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь используем **SFTTrainer** из библиотеки Huggingface TRL! Подробнее в документации: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer).\n",
        "\n",
        "Мы проведем всего 60 шагов во время дообучения, но вы можете установить num_train_epochs=1 для полноценного обучения и отключить max_steps=None.\n",
        "\n",
        "Также есть поддержка DPOTrainer от TRL!"
      ],
      "metadata": {
        "id": "CxzcpeHoD-0T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    # compute_metrics=compute_metrics,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 2,\n",
        "        max_steps = 60,\n",
        "        # num_train_epochs = 3, # Для длительного дообучения\n",
        "        learning_rate = 0.001,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "#@title Состояние системы\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "# Отключение логирования wandb\n",
        "wandb.init(mode='disabled')"
      ],
      "metadata": {
        "id": "_DxZjDIx4xsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запуск дообучения"
      ],
      "metadata": {
        "id": "pa4CD_8NFPtl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "#@title Показать итоговую статистику по расходу памяти и времени\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "### Инференс модели"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим, как модель генерирует текст после дообучения"
      ],
      "metadata": {
        "id": "D7ANkY9VFtFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here. I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it. How can I change my feeling of being worthless to everyone?\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Время начала генерации\n",
        "start_time = time.time()\n",
        "# Генерируем текст\n",
        "output_ids = model.generate(input_ids, max_new_tokens=150, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# Время окончания генерации\n",
        "end_time = time.time()\n",
        "\n",
        "# Время, затраченное на генерацию\n",
        "generation_time = end_time - start_time\n",
        "print(f\"Время генерации: {generation_time:.2f} секунд\")"
      ],
      "metadata": {
        "id": "wLxej7hKpArx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Декодируем токены в строку\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "f0-3kuu4pAry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "### Сохранение и загрузка дообученных моделей\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Сохранение только LoRA весовых коэффициентов\n",
        "\n",
        "Чтобы сохранить итоговую модель в виде LoRA адаптеров, используйте push_to_hub от Huggingface для сохранения на HF или save_pretrained для локального сохранения.\n",
        "\n",
        "[ПРИМЕЧАНИЕ] Это сохраняет только LoRA адаптеры, а не всю модель.\n",
        "Чтобы сохранить модель в формате 16bit или GGUF, прокрутите вниз!"
      ],
      "metadata": {
        "id": "7nBzK5n2GbYv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "\n",
        "Теперь, если вы хотите загрузить адаптеры LoRA, которые мы только что сохранили для выводов,  `False` замените на `True`:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # ваша дообученная модель\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)\n",
        "pass\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Describe anything special about a sequence. Your input is 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOFzC441vCtq"
      },
      "source": [
        "<a name=\"Ollama\"></a>\n",
        "#### Ollama Support\n",
        "\n",
        "Unsloth позволяет дообучать модели, создавать Modelfile и экспортировать их в Ollama! Это значительно упрощает процесс дообучения и обеспечивает плавный рабочий процесс от Unsloth к Ollama!\n",
        "\n",
        "Давайте сначала установим Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUxcyP_UfeLl"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь сохраним модель в формате GGUF\n",
        "\n",
        "Используйте метод save_pretrained_gguf для локального сохранения модели и push_to_hub_gguf для загрузки на Hugging Face Hub.\n",
        "По умолчанию модель сохраняется в формате q8_0.\n",
        "\n",
        "Некоторые поддерживаемые методы квантования, больше - [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options):\n",
        "\n",
        "q8_0 — Быстрая конвертация. Требует много ресурсов, но в целом приемлемо.\n",
        "\n",
        "q4_k_m — Рекомендуемый метод. Использует Q6_K для половины тензоров attention.wv и feed_forward.w2, остальное — Q4_K.\n",
        "\n",
        "q5_k_m — Рекомендуемый метод. Использует Q6_K для половины тензоров attention.wv и feed_forward.w2, остальное — Q5_K."
      ],
      "metadata": {
        "id": "uXYwqFE0HesT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if True: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# За HF токеном идем -  https://huggingface.co/settings/tokens\n",
        "# Измени hf на своем имя!\n",
        "# Создай в своем профиле модель с именем model , чтобы сохранить в нее модель\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Сохранить в 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Сохранить в q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Сохранить в различные форматы GGUF\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Измени hf на своем имя!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # За HF токеном идем -  https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Загрузка модели на HF"
      ],
      "metadata": {
        "id": "y0OR0Eas_4xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if True: model.push_to_hub_gguf(\"katya1836/model\", tokenizer, token = \"\")"
      ],
      "metadata": {
        "id": "guMzFT-Cssfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установка и запуск Ollama с помощью Python"
      ],
      "metadata": {
        "id": "t_U6PkDFJHnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()"
      ],
      "metadata": {
        "id": "ZYcXi_7e_BKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull hf.co/katya1836/model"
      ],
      "metadata": {
        "id": "eW9ZYob71R-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"\n",
        "### Question:\n",
        "I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here. I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it. How can I change my feeling of being worthless to everyone?\n",
        "\n",
        "### Corresponding answer:\n",
        "\"\"\"\n",
        "\n",
        "response = requests.post(\n",
        "    'http://localhost:11434/api/generate',\n",
        "    json={\n",
        "        'model': \"hf.co/katya1836/model\",\n",
        "        \"options\": { \"temperature\": 0, \"num_predict\": 150},\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": False\n",
        "    }\n",
        ")\n",
        "\n",
        "# Декодируем содержимое ответа\n",
        "result = response.content.decode('utf-8')\n",
        "result_json = json.loads(result)\n",
        "# Извлекаем текст из ключа 'response'\n",
        "generated_text = result_json.get('response')\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "xXhPV-Um_Eqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Загрузка модели на Ollama"
      ],
      "metadata": {
        "id": "70nNbJioLb7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь мы можем выполнить инференс с использованием этой модели через Ollama!"
      ],
      "metadata": {
        "id": "_jVrVcb5LEl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()"
      ],
      "metadata": {
        "id": "CdXh5UMNgLN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ollama требует файл модели (Modelfile), который указывает формат подсказок (prompts) для модели. Давайте выведем автоматически сгенерированный файл модели от Unsloth"
      ],
      "metadata": {
        "id": "9Xm_EAe6J_cZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h82vfNigRhiz"
      },
      "outputs": [],
      "source": [
        "print(tokenizer._ollama_modelfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь мы создадим модель Ollama под названием unsloth_model, используя Modelfile, который был автоматически сгенерирован!"
      ],
      "metadata": {
        "id": "ZTQYHLv1KJzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama create unsloth_model -f ./model/Modelfile"
      ],
      "metadata": {
        "id": "wNijsEumiywt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama run unsloth_model"
      ],
      "metadata": {
        "id": "tYaXy24XgLN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"\n",
        "### Question:\n",
        "I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here. I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it. How can I change my feeling of being worthless to everyone?\n",
        "\n",
        "### Corresponding answer:\n",
        "\"\"\"\n",
        "\n",
        "response = requests.post(\n",
        "    'http://localhost:11434/api/generate',\n",
        "    json={\n",
        "        'model': \"unsloth_model\",\n",
        "        \"options\": { \"temperature\": 0, \"num_predict\": 150},\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": False\n",
        "    }\n",
        ")\n",
        "\n",
        "# Декодируем содержимое ответа\n",
        "result = response.content.decode('utf-8')\n",
        "result_json = json.loads(result)\n",
        "# Извлекаем текст из ключа 'response'\n",
        "generated_text = result_json.get('response')\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "eLa82OyigLN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Имитация работы через командную строку"
      ],
      "metadata": {
        "id": "sfF3tBBZKUPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import JSON\n",
        "from google.colab import output\n",
        "from subprocess import getoutput\n",
        "import os\n",
        "\n",
        "def shell(command):\n",
        "  if command.startswith('cd'):\n",
        "    path = command.strip().split(maxsplit=1)[1]\n",
        "    os.chdir(path)\n",
        "    return JSON([''])\n",
        "  return JSON([getoutput(command)])\n",
        "output.register_callback('shell', shell)"
      ],
      "metadata": {
        "id": "q0hyOUsRPs7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Colab Shell\n",
        "%%html\n",
        "<div id=term_demo></div>\n",
        "<script src=\"https://code.jquery.com/jquery-latest.js\"></script>\n",
        "<script src=\"https://cdn.jsdelivr.net/npm/jquery.terminal/js/jquery.terminal.min.js\"></script>\n",
        "<link href=\"https://cdn.jsdelivr.net/npm/jquery.terminal/css/jquery.terminal.min.css\" rel=\"stylesheet\"/>\n",
        "<script>\n",
        "  $('#term_demo').terminal(async function(command) {\n",
        "      if (command !== '') {\n",
        "          try {\n",
        "              let res = await google.colab.kernel.invokeFunction('shell', [command])\n",
        "              let out = res.data['application/json'][0]\n",
        "              this.echo(new String(out))\n",
        "          } catch(e) {\n",
        "              this.error(new String(e));\n",
        "          }\n",
        "      } else {\n",
        "          this.echo('');\n",
        "      }\n",
        "  }, {\n",
        "      greetings: 'Welcome to Colab Shell',\n",
        "      name: 'colab_demo',\n",
        "      height: 250,\n",
        "      prompt: 'colab > '\n",
        "  });"
      ],
      "metadata": {
        "id": "TEMABh7QLLDK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
