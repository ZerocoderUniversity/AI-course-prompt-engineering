# Fine-Tuning моделей с использованием UnsloTh

Этот проект демонстрирует процесс настройки (fine-tuning) современных моделей с использованием библиотеки UnsloTh. Проект разработан для оптимизации работы языковых моделей, таких как Llama, Vicuna, и других, с целью улучшения производительности в специфичных задачах, например, в области поддержки психического здоровья или разговорных моделей.

## Основные возможности проекта
- **Поддержка современных моделей:**
  - Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes и другие.
  - Возможность работы с 16-битным LoRA и 4-битным QLoRA для ускорения и экономии ресурсов.
- **Гибкость настройки:**
  - Возможность задавать максимальную длину последовательности (`max_seq_length`) с автоматическим масштабированием.
- **Оптимизация производительности:**
  - Ускоренная работа Phi-3 Medium/Mini в два раза благодаря последним обновлениям.

## Подготовка данных
Для обучения используется предварительно обработанный набор данных [Mental Health Counseling Conversations](https://huggingface.co/datasets/Amod/mental_health_counseling_conversations). Данный набор представлен в формате, совместимом с библиотекой `datasets` от Hugging Face.

Пример кода для загрузки:
```python
from datasets import load_dataset

data = load_dataset("Amod/mental_health_counseling_conversations")
```

## Установка
1. Установите библиотеку UnsloTh с помощью следующих команд:
   ```bash
   pip install unsloth
   # Для последней версии:
   pip uninstall unsloth -y && pip install --upgrade --no-cache-dir "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
   ```

2. Убедитесь, что у вас установлены дополнительные зависимости для работы с моделями.

## Настройка модели
### Добавление LoRA для обновления весов
LoRA (Low-Rank Adaptation) используется для обновления части весов модели, что снижает требования к вычислительным ресурсам и ускоряет обучение.
Пример кода:
```python
from unsloth import ModelTrainer

trainer = ModelTrainer(model="llama",
                       dataset="Amod/mental_health_counseling_conversations",
                       method="lora")

trainer.train()
```

### Параметры обучения
- **Метод обучения:** LoRA или QLoRA.
- **Параметры модели:** Возможность установки максимальной длины последовательности и автоматической оптимизации.

## Преимущества
- **Экономия ресурсов:** Использование методов LoRA и QLoRA позволяет значительно снизить объем памяти и времени, необходимого для обучения.
- **Масштабируемость:** Поддержка множества моделей и автоматическое масштабирование параметров.
- **Удобство использования:** Простая интеграция с популярными библиотеками, такими как Hugging Face Datasets.

## Возможные улучшения
- Добавить поддержку большего числа моделей и методов оптимизации.
- Улучшить документацию для упрощения работы с библиотекой.
- Интеграция с другими инструментами для анализа производительности моделей.

## Заключение
Этот проект является отличным примером использования современных инструментов для настройки языковых моделей. Он демонстрирует навыки работы с AI, оптимизации моделей и обработки данных, что делает его ценным дополнением в портфолио разработчика.


